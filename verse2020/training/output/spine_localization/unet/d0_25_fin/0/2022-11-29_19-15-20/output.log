using pyro uri PYRO:verse2020_dataset@s146.uppmax.uu.se:52132
loaded 37 ids
loaded 37 ids
Starting main loop
Training parameters:
Optimizer:  <tensorflow.python.keras.mixed_precision.experimental.loss_scale_optimizer.LossScaleOptimizer object at 0x2b6f2fef44a8>
Batch size: 1
Learning rate: <tensorflow.python.keras.optimizer_v2.learning_rate_schedule.ExponentialDecay object at 0x2b6f2fef4400>
Max iterations: 10000
Output folder: ./output/spine_localization/unet/d0_25_fin/0/2022-11-29_19-15-20
19:15:48: train iter: 0 loss_net: 0.3050 loss_scale: 32768.0000 norm: 5.6692 norm_average: 9.9567 seconds: 24.986
19:16:16: train iter: 100 loss_net: 0.9960 loss_scale: 1050.5601 norm: inf norm_average: 17.5132 seconds: 27.857
19:16:32: train iter: 200 loss_net: 0.4579 loss_scale: 32.0000 norm: 68.7855 norm_average: 30.0313 seconds: 16.664
19:16:51: train iter: 300 loss_net: 0.3546 loss_scale: 32.0000 norm: 52.4604 norm_average: 36.1318 seconds: 18.615
19:17:12: train iter: 400 loss_net: nan loss_scale: 29.2800 norm: nan norm_average: 40.6065 seconds: 21.292
19:17:35: train iter: 500 loss_net: nan loss_scale: 15.9200 norm: nan norm_average: 38.8392 seconds: 22.365
19:17:55: train iter: 600 loss_net: 0.2041 loss_scale: 8.0000 norm: 33.8449 norm_average: 35.9372 seconds: 20.459
19:18:15: train iter: 700 loss_net: 0.1718 loss_scale: 8.0000 norm: 26.6169 norm_average: 31.6654 seconds: 19.930
19:18:36: train iter: 800 loss_net: 0.1537 loss_scale: 8.0000 norm: 26.7457 norm_average: 28.5855 seconds: 21.175
19:18:56: train iter: 900 loss_net: 0.1280 loss_scale: 8.0000 norm: 22.2089 norm_average: 25.6768 seconds: 19.540
19:19:15: train iter: 1000 loss_net: 0.1338 loss_scale: 8.0000 norm: 21.6625 norm_average: 23.0893 seconds: 19.459
19:19:37: train iter: 1100 loss_net: 0.1188 loss_scale: 8.0000 norm: 19.6374 norm_average: 20.4324 seconds: 21.790
19:19:57: train iter: 1200 loss_net: 0.1161 loss_scale: 8.0000 norm: 20.7255 norm_average: 20.4321 seconds: 20.347
19:20:17: train iter: 1300 loss_net: 0.1151 loss_scale: 8.0000 norm: 20.7485 norm_average: 20.1832 seconds: 19.406
19:20:39: train iter: 1400 loss_net: 0.1251 loss_scale: 8.0000 norm: 22.7237 norm_average: 20.8406 seconds: 22.193
19:21:01: train iter: 1500 loss_net: 0.0884 loss_scale: 8.0800 norm: 16.3275 norm_average: 19.7167 seconds: 21.841
19:21:23: train iter: 1600 loss_net: 0.0883 loss_scale: 16.0000 norm: 17.0775 norm_average: 18.1138 seconds: 22.068
19:21:44: train iter: 1700 loss_net: 0.0835 loss_scale: 16.0000 norm: 16.0936 norm_average: 16.2249 seconds: 21.072
19:22:07: train iter: 1800 loss_net: nan loss_scale: 8.4800 norm: nan norm_average: 14.9664 seconds: 22.690
19:22:28: train iter: 1900 loss_net: 0.0662 loss_scale: 8.0000 norm: 11.2082 norm_average: 13.1014 seconds: 21.015
19:22:48: train iter: 2000 loss_net: 0.0751 loss_scale: 8.0000 norm: 14.4491 norm_average: 13.1844 seconds: 20.804
19:23:09: train iter: 2100 loss_net: 0.0616 loss_scale: 8.0000 norm: 10.5984 norm_average: 12.6845 seconds: 20.547
19:23:30: train iter: 2200 loss_net: nan loss_scale: 5.6000 norm: nan norm_average: 11.5645 seconds: 21.352
19:23:49: train iter: 2300 loss_net: 0.0700 loss_scale: 4.0000 norm: 13.0064 norm_average: 12.2462 seconds: 18.791
19:24:07: train iter: 2400 loss_net: 0.0700 loss_scale: 4.0000 norm: 14.1500 norm_average: 12.5411 seconds: 18.258
19:24:25: train iter: 2500 loss_net: 0.0458 loss_scale: 4.0000 norm: 9.4945 norm_average: 12.3395 seconds: 18.035
19:24:45: train iter: 2600 loss_net: 0.0622 loss_scale: 4.0000 norm: 10.6383 norm_average: 10.8416 seconds: 20.124
19:25:09: train iter: 2700 loss_net: nan loss_scale: 3.8800 norm: nan norm_average: 10.9427 seconds: 23.366
19:25:27: train iter: 2800 loss_net: 0.0606 loss_scale: 2.0000 norm: 11.4762 norm_average: 11.4291 seconds: 18.396
19:25:47: train iter: 2900 loss_net: nan loss_scale: 1.0700 norm: nan norm_average: 10.8624 seconds: 20.244
19:26:06: train iter: 3000 loss_net: 0.0475 loss_scale: 1.0000 norm: 10.1681 norm_average: 9.9680 seconds: 18.028
19:26:27: train iter: 3100 loss_net: 0.0544 loss_scale: 1.0000 norm: 11.2536 norm_average: 10.0461 seconds: 21.421
19:26:47: train iter: 3200 loss_net: 0.0551 loss_scale: 1.0000 norm: 10.5699 norm_average: 10.5988 seconds: 19.847
19:27:06: train iter: 3300 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 10.3100 seconds: 19.231
19:27:25: train iter: 3400 loss_net: 0.0488 loss_scale: 1.0000 norm: 10.0405 norm_average: 9.6820 seconds: 19.472
19:27:46: train iter: 3500 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 9.8289 seconds: 20.340
19:28:07: train iter: 3600 loss_net: 0.0406 loss_scale: 1.0000 norm: 9.1967 norm_average: 9.9303 seconds: 20.796
19:28:28: train iter: 3700 loss_net: 0.0423 loss_scale: 1.0000 norm: 9.2849 norm_average: 8.9093 seconds: 21.168
19:28:53: train iter: 3800 loss_net: 0.0530 loss_scale: 1.0000 norm: 11.8953 norm_average: 10.3584 seconds: 25.113
19:29:13: train iter: 3900 loss_net: 0.0459 loss_scale: 1.0000 norm: 11.1296 norm_average: 11.4635 seconds: 20.142
19:29:33: train iter: 4000 loss_net: 0.0527 loss_scale: 1.0000 norm: 11.2005 norm_average: 11.0340 seconds: 19.716
19:29:55: train iter: 4100 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 9.7952 seconds: 21.811
19:30:18: train iter: 4200 loss_net: 0.0410 loss_scale: 1.0000 norm: 11.1960 norm_average: 10.3321 seconds: 23.550
19:30:40: train iter: 4300 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 10.7591 seconds: 21.852
19:31:01: train iter: 4400 loss_net: 0.0391 loss_scale: 1.0000 norm: 11.0421 norm_average: 10.8731 seconds: 20.794
19:31:21: train iter: 4500 loss_net: 0.0509 loss_scale: 1.0000 norm: 10.6798 norm_average: 10.7198 seconds: 20.165
19:31:39: train iter: 4600 loss_net: 0.0344 loss_scale: 1.0000 norm: 8.9034 norm_average: 10.1099 seconds: 18.413
19:32:00: train iter: 4700 loss_net: 0.0349 loss_scale: 1.0000 norm: 9.8258 norm_average: 10.0553 seconds: 20.549
19:32:21: train iter: 4800 loss_net: 0.0417 loss_scale: 1.0000 norm: 10.4358 norm_average: 9.6585 seconds: 21.567
19:32:42: train iter: 4900 loss_net: 0.0519 loss_scale: 1.0000 norm: 13.3889 norm_average: 10.9187 seconds: 20.374
Creating snapshot...
Model saved in file ./output/spine_localization/unet/d0_25_fin/0/2022-11-29_19-15-20/weights/ckpt-5000
Testing...
verse518
verse406_verse214_CT-sag
verse506_CT-iso
verse402_verse251_CT-sag
verse605_CT-sag
verse034
verse411_verse270_CT-iso
verse823_CT-iso
verse833_CT-ax
verse820_CT-iso
verse564_CT-iso
verse254
verse504
GL047
verse104_CT-iso
verse407_verse262_CT-sag
verse406_verse261_CT-sag
verse533
verse542_CT-iso
verse113
verse091
verse033
verse008
GL003
verse411_verse232_CT-iso
verse544
verse539_CT-iso
GL016
verse015
verse402_verse202_CT-sag
GL124_CT-ax
verse503
verse527
verse122
verse407_verse215_CT-sag
verse557
verse594
19:34:47: test iter: 5000 loss_net: 0.0299 mean_iou: 0.8185 seconds: 1164.674
19:34:48: train iter: 5000 loss_net: 0.0368 loss_scale: 1.0000 norm: 12.4183 norm_average: 12.7036 seconds: 125.816
19:35:05: train iter: 5100 loss_net: 0.0398 loss_scale: 1.0000 norm: 10.6215 norm_average: 11.4151 seconds: 17.327
19:35:23: train iter: 5200 loss_net: 0.0388 loss_scale: 1.0000 norm: 10.1680 norm_average: 10.8717 seconds: 17.644
19:35:40: train iter: 5300 loss_net: 0.0359 loss_scale: 1.0100 norm: 9.7592 norm_average: 10.1215 seconds: 16.902
19:36:00: train iter: 5400 loss_net: nan loss_scale: 1.0800 norm: nan norm_average: 10.8433 seconds: 20.028
19:36:19: train iter: 5500 loss_net: 0.0436 loss_scale: 1.0000 norm: 10.4963 norm_average: 10.6921 seconds: 19.867
19:36:38: train iter: 5600 loss_net: 0.0380 loss_scale: 1.0000 norm: 11.6416 norm_average: 11.4231 seconds: 18.667
19:36:56: train iter: 5700 loss_net: 0.0362 loss_scale: 1.0000 norm: 10.1647 norm_average: 10.4617 seconds: 17.680
19:37:16: train iter: 5800 loss_net: 0.0310 loss_scale: 1.0000 norm: 9.8686 norm_average: 10.0502 seconds: 20.142
19:37:39: train iter: 5900 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 9.6040 seconds: 23.557
19:38:02: train iter: 6000 loss_net: 0.0333 loss_scale: 1.0000 norm: 9.3234 norm_average: 9.2765 seconds: 22.109
19:38:22: train iter: 6100 loss_net: 0.0292 loss_scale: 1.0000 norm: 9.7887 norm_average: 9.7787 seconds: 20.045
19:38:40: train iter: 6200 loss_net: 0.0455 loss_scale: 1.0000 norm: 13.7652 norm_average: 11.0258 seconds: 18.091
19:39:00: train iter: 6300 loss_net: 0.0511 loss_scale: 1.0000 norm: 15.2622 norm_average: 13.6314 seconds: 19.998
19:39:23: train iter: 6400 loss_net: 0.0369 loss_scale: 1.0000 norm: 11.7423 norm_average: 13.2945 seconds: 23.350
19:39:44: train iter: 6500 loss_net: 0.0340 loss_scale: 1.0000 norm: 11.1565 norm_average: 12.1486 seconds: 21.357
19:40:07: train iter: 6600 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 10.9785 seconds: 22.228
19:40:29: train iter: 6700 loss_net: 0.0442 loss_scale: 1.0000 norm: 15.4806 norm_average: 12.2142 seconds: 21.960
19:40:50: train iter: 6800 loss_net: 0.0362 loss_scale: 1.0000 norm: 13.3261 norm_average: 13.7160 seconds: 21.304
19:41:12: train iter: 6900 loss_net: 0.0333 loss_scale: 1.0000 norm: 10.8162 norm_average: 12.8326 seconds: 21.736
19:41:31: train iter: 7000 loss_net: 0.0347 loss_scale: 1.0000 norm: 12.4277 norm_average: 12.1580 seconds: 19.583
19:41:54: train iter: 7100 loss_net: 0.0315 loss_scale: 1.0000 norm: 11.3094 norm_average: 12.0639 seconds: 22.511
19:42:14: train iter: 7200 loss_net: 0.0339 loss_scale: 1.0000 norm: 12.4510 norm_average: 11.8406 seconds: 20.527
19:42:35: train iter: 7300 loss_net: 0.0351 loss_scale: 1.0000 norm: 10.9648 norm_average: 11.1027 seconds: 21.067
19:42:54: train iter: 7400 loss_net: 0.0287 loss_scale: 1.0000 norm: 10.5198 norm_average: 11.1318 seconds: 18.873
19:43:16: train iter: 7500 loss_net: 0.0383 loss_scale: 1.0000 norm: 13.9867 norm_average: 12.0461 seconds: 21.548
19:43:38: train iter: 7600 loss_net: 0.0292 loss_scale: 1.0700 norm: 11.1008 norm_average: 12.1530 seconds: 21.817
19:43:58: train iter: 7700 loss_net: 0.0346 loss_scale: 2.0000 norm: 13.3525 norm_average: 12.2463 seconds: 20.284
19:44:20: train iter: 7800 loss_net: 0.0254 loss_scale: 2.0000 norm: 9.6254 norm_average: 11.4956 seconds: 22.056
19:44:39: train iter: 7900 loss_net: 0.0252 loss_scale: 2.0000 norm: 11.3501 norm_average: 10.6284 seconds: 19.407
19:45:02: train iter: 8000 loss_net: 0.0337 loss_scale: 2.0000 norm: 13.4087 norm_average: 11.5954 seconds: 23.040
19:45:21: train iter: 8100 loss_net: 0.0385 loss_scale: 2.0000 norm: 11.5898 norm_average: 12.5412 seconds: 18.730
19:45:40: train iter: 8200 loss_net: nan loss_scale: 1.1500 norm: nan norm_average: 12.7807 seconds: 19.264
19:46:03: train iter: 8300 loss_net: 0.0278 loss_scale: 1.0000 norm: 12.0037 norm_average: 13.2411 seconds: 22.265
19:46:24: train iter: 8400 loss_net: 0.0386 loss_scale: 1.0000 norm: 14.1456 norm_average: 13.1194 seconds: 21.802
19:46:45: train iter: 8500 loss_net: 0.0341 loss_scale: 1.0000 norm: 10.7951 norm_average: 12.1979 seconds: 20.975
19:47:07: train iter: 8600 loss_net: 0.0378 loss_scale: 1.0000 norm: 13.9316 norm_average: 12.1614 seconds: 21.224
19:47:28: train iter: 8700 loss_net: 0.0253 loss_scale: 1.0000 norm: 10.3050 norm_average: 12.5394 seconds: 21.197
19:47:48: train iter: 8800 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 12.1708 seconds: 20.307
19:48:06: train iter: 8900 loss_net: 0.0312 loss_scale: 1.0000 norm: 11.8252 norm_average: 12.2415 seconds: 18.289
19:48:28: train iter: 9000 loss_net: 0.0282 loss_scale: 1.0000 norm: 14.1625 norm_average: 12.5367 seconds: 21.298
19:48:49: train iter: 9100 loss_net: 0.0273 loss_scale: 1.0000 norm: 12.4383 norm_average: 13.2747 seconds: 21.516
19:49:12: train iter: 9200 loss_net: 0.0308 loss_scale: 1.0000 norm: 13.0076 norm_average: 12.9393 seconds: 22.869
19:49:35: train iter: 9300 loss_net: 0.0297 loss_scale: 1.0000 norm: 12.9541 norm_average: 12.7657 seconds: 22.420
19:49:59: train iter: 9400 loss_net: 0.0333 loss_scale: 1.0000 norm: 14.9201 norm_average: 13.9505 seconds: 23.977
19:50:20: train iter: 9500 loss_net: nan loss_scale: 1.0000 norm: nan norm_average: 15.1372 seconds: 21.344
19:50:42: train iter: 9600 loss_net: 0.0300 loss_scale: 1.0000 norm: 14.8455 norm_average: 15.5589 seconds: 22.109
19:51:05: train iter: 9700 loss_net: 0.0337 loss_scale: 1.0000 norm: 14.7078 norm_average: 14.7535 seconds: 23.031
19:51:23: train iter: 9800 loss_net: 0.0326 loss_scale: 1.0000 norm: 14.6944 norm_average: 14.6509 seconds: 18.303
19:51:44: train iter: 9900 loss_net: 0.0345 loss_scale: 1.0000 norm: 14.7513 norm_average: 13.9610 seconds: 20.964
Creating snapshot...
Model saved in file ./output/spine_localization/unet/d0_25_fin/0/2022-11-29_19-15-20/weights/ckpt-10000
Testing...
verse518
verse406_verse214_CT-sag
verse506_CT-iso
verse402_verse251_CT-sag
verse605_CT-sag
verse034
verse411_verse270_CT-iso
verse823_CT-iso
verse833_CT-ax
verse820_CT-iso
verse564_CT-iso
verse254
verse504
GL047
verse104_CT-iso
verse407_verse262_CT-sag
verse406_verse261_CT-sag
verse533
verse542_CT-iso
verse113
verse091
verse033
verse008
GL003
verse411_verse232_CT-iso
verse544
verse539_CT-iso
GL016
verse015
verse402_verse202_CT-sag
GL124_CT-ax
verse503
verse527
verse122
verse407_verse215_CT-sag
verse557
verse594
19:53:41: test iter: 10000 loss_net: 0.0278 mean_iou: 0.8380 seconds: 1133.385
